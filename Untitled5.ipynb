{
 "metadata": {
  "name": "",
  "signature": "sha256:aae2067faad0eb522285f97c17b630c37c4f70b3b73a5ad31e991fcfc6af2de6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So if we let the output of the softmax be $p_i$,\n",
      "$$p_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} $$\n",
      "\n",
      "and we had $N$ different softmax outputs to do a geometric mean over, we have:\n",
      "$$\n",
      "\\begin{align*}\n",
      "\\hat{p}_i = \\sqrt[N]{\\prod^N_k p_{ki}} &= \\frac{\\sqrt[N]{\\prod_k \\exp(x_{ki})}}{\\sqrt[N]{\\prod_k \\sum_j \\exp(x_{kj})}}\\\\\n",
      " &= \\frac{\\exp\\left(\\frac{1}{N}\\sum^N_k x_{ki}\\right)}{\\sqrt[N]{\\prod_k \\sum_j \\exp(x_{kj})}}\n",
      "\\end{align*}\n",
      "$$\n",
      "if we want to renormalise this geometric mean of probabilities to 1, then we get that taking the mean of the linear outputs is the same as taking the geometric mean of the softmax.\n",
      "$$\\frac{\\hat{p}_i}{\\sum_j \\hat{p}_j}  =  \\frac{\\exp\\left(\\frac{1}{N}\\sum^N_k x_{ki}\\right)}{\\sum_j \\exp\\left(\\frac{1}{N}\\sum^N_k x_{kj}\\right)}$$"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}